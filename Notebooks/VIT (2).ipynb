{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_sALBA1c4lL",
        "outputId": "d36909cd-f368-4efb-e4f3-8e6467163ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.13.1+cu116 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.13.1+cu116\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2335331729.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# üóÇÔ∏è STEP 3: Set paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "# üß© STEP 1: Install dependencies\n",
        "!pip install -q torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "!pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
        "\n",
        "# üß† STEP 2: Import packages and mount Google Drive\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# üóÇÔ∏è STEP 3: Set paths\n",
        "INPUT_VIDEO_DIR = \"/content/drive/MyDrive/Test\"      # Folder with input .avi files\n",
        "OUTPUT_VIDEO_DIR = \"/content/drive/MyDrive/SegmentedTest\"        # Folder for cropped outputs\n",
        "SAM_CHECKPOINT_PATH = \"/content/drive/MyDrive/sam_vit_b.pth\"      # SAM model checkpoint path\n",
        "\n",
        "os.makedirs(OUTPUT_VIDEO_DIR, exist_ok=True)\n",
        "\n",
        "# üß† STEP 4: Load SAM\n",
        "sam = sam_model_registry[\"vit_b\"](checkpoint=SAM_CHECKPOINT_PATH)\n",
        "sam.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "# üß† STEP 5: Function to get bounding box from first frame using SAM\n",
        "def get_bbox_from_first_frame(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    ret, first_frame = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "    if not ret:\n",
        "        print(f\"‚ùå Couldn't read first frame from {video_path}\")\n",
        "        return None\n",
        "\n",
        "    rgb = cv2.cvtColor(first_frame, cv2.COLOR_BGR2RGB)\n",
        "    predictor.set_image(rgb)\n",
        "\n",
        "    h, w = rgb.shape[:2]\n",
        "    input_point = np.array([[w // 2, h // 2]])\n",
        "    input_label = np.array([1])\n",
        "\n",
        "    masks, _, _ = predictor.predict(\n",
        "        point_coords=input_point,\n",
        "        point_labels=input_label,\n",
        "        multimask_output=False\n",
        "    )\n",
        "\n",
        "    mask = masks[0].astype(np.uint8)\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if not contours:\n",
        "        print(f\"‚ö†Ô∏è No contours found in first frame of {video_path}\")\n",
        "        return None\n",
        "\n",
        "    x, y, bw, bh = cv2.boundingRect(contours[0])\n",
        "    return (x, y, bw, bh)\n",
        "\n",
        "# üéûÔ∏è STEP 6: Crop video using bounding box\n",
        "def crop_video(video_path, output_path, bbox):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    x, y, w, h = bbox\n",
        "\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        cropped = frame[y:y+h, x:x+w]\n",
        "        out.write(cropped)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "# üîÅ STEP 7: Batch process all videos\n",
        "for file in os.listdir(INPUT_VIDEO_DIR):\n",
        "    if file.lower().endswith(\".avi\"):\n",
        "        input_path = os.path.join(INPUT_VIDEO_DIR, file)\n",
        "        output_path = os.path.join(OUTPUT_VIDEO_DIR, f\"cropped_{file}\")\n",
        "\n",
        "        print(f\"‚ñ∂Ô∏è Processing: {file}\")\n",
        "        bbox = get_bbox_from_first_frame(input_path)\n",
        "\n",
        "        if bbox:\n",
        "            crop_video(input_path, output_path, bbox)\n",
        "            print(f\"‚úÖ Done: {output_path}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Skipped: {file} (no valid bbox)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "collapsed": true,
        "id": "ceJRnPKM2Awr",
        "outputId": "33729613-267b-4719-8ed9-d8d1ce084d88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 330M/330M [00:05<00:00, 61.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing training data...\n",
            "Processing pure training videos...\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (24).avi, Total frames: 490\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (24).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (24).avi, Total frames: 450\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (24).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (23).avi, Total frames: 490\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (23).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (23).avi, Total frames: 450\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (23).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (23).avi, Total frames: 450\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (23).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (22).avi, Total frames: 450\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (22).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (21).avi, Total frames: 490\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (21).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (21).avi, Total frames: 490\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (21).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (20).avi, Total frames: 450\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (20).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (20).avi, Total frames: 490\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (20).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (20).avi, Total frames: 340\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (20).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (19).avi, Total frames: 460\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (19).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (19).avi, Total frames: 490\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (19).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (19).avi, Total frames: 310\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (19).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (18).avi, Total frames: 480\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (18).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (18).avi, Total frames: 490\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (18).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (17).avi, Total frames: 390\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (17).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (17).avi, Total frames: 490\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (17).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (17).avi, Total frames: 220\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (17).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (16).avi, Total frames: 100\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (16).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (16).avi, Total frames: 411\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (16).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (16).avi, Total frames: 430\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (16).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (15).avi, Total frames: 491\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (15).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (15).avi, Total frames: 370\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (15).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (15).avi, Total frames: 420\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (15).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (14).avi, Total frames: 496\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (14).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (14).avi, Total frames: 310\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (14).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (14).avi, Total frames: 415\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (14).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (13).avi, Total frames: 471\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (13).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (13).avi, Total frames: 491\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (13).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (13).avi, Total frames: 491\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (13).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (12).avi, Total frames: 360\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (12).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (12).avi, Total frames: 486\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (12).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (11).avi, Total frames: 175\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (11).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (11).avi, Total frames: 495\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (11).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (10).avi, Total frames: 486\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (10).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (10).avi, Total frames: 210\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (10).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (10).avi, Total frames: 380\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (10).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (9).avi, Total frames: 491\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (9).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (9).avi, Total frames: 330\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (9).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (8).avi, Total frames: 360\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_35-60 (8).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (8).avi, Total frames: 275\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (8).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (6).avi, Total frames: 405\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_65-90 (6).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (6).avi, Total frames: 385\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPT/cropped_95-110 (6).avi\n",
            "Finished processing pure training videos.\n",
            "Processing adulterated training videos...\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoAT/cropped_35-60 (7).avi, Total frames: 485\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoAT/cropped_35-60 (7).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoAT/cropped_65-90 (7).avi, Total frames: 460\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoAT/cropped_65-90 (7).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoAT/cropped_35-60 (6).avi, Total frames: 480\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoAT/cropped_35-60 (6).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoAT/cropped_65-90 (6).avi, Total frames: 380\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoAT/cropped_65-90 (6).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoAT/cropped_95-110 (6).avi, Total frames: 315\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoAT/cropped_95-110 (6).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoAT/cropped_35-60 (5).avi, Total frames: 480\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoAT/cropped_35-60 (5).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoAT/cropped_65-90 (5).avi, Total frames: 361\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoAT/cropped_65-90 (5).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoAT/cropped_35-60 (4).avi, Total frames: 496\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoAT/cropped_35-60 (4).avi\n",
            "Finished processing adulterated training videos.\n",
            "Number of training data samples: 52\n",
            "Epoch [100/200], Loss: 0.2181\n",
            "Epoch [200/200], Loss: 0.1363\n",
            "Preparing validation data...\n",
            "Processing pure validation videos...\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPV/cropped_65-90.avi, Total frames: 275\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPV/cropped_65-90.avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPV/cropped_95-110 (3).avi, Total frames: 300\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPV/cropped_95-110 (3).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPV/cropped_65-90 (1).avi, Total frames: 475\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPV/cropped_65-90 (1).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPV/cropped_95-110 (1).avi, Total frames: 470\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPV/cropped_95-110 (1).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPV/cropped_35-60 (1).avi, Total frames: 486\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPV/cropped_35-60 (1).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPV/cropped_35-60.avi, Total frames: 360\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPV/cropped_35-60.avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPV/cropped_65-90 (2).avi, Total frames: 460\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPV/cropped_65-90 (2).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPV/cropped_35-60 (2).avi, Total frames: 480\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPV/cropped_35-60 (2).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPV/cropped_35-60 (3).avi, Total frames: 480\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPV/cropped_35-60 (3).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoPV/cropped_65-90 (3).avi, Total frames: 435\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoPV/cropped_65-90 (3).avi\n",
            "Finished processing pure validation videos.\n",
            "Processing adulterated validation videos...\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoAV/cropped_65-90 (2).avi, Total frames: 481\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoAV/cropped_65-90 (2).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoAV/cropped_35-60 (1).avi, Total frames: 490\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoAV/cropped_35-60 (1).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoAV/cropped_65-90 (1).avi, Total frames: 495\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoAV/cropped_65-90 (1).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoAV/cropped_95-110 (1).avi, Total frames: 485\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoAV/cropped_95-110 (1).avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoAV/cropped_35-60.avi, Total frames: 495\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoAV/cropped_35-60.avi\n",
            "Processing video: /content/drive/MyDrive/SegmentedVideoAV/cropped_65-90.avi, Total frames: 410\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedVideoAV/cropped_65-90.avi\n",
            "Finished processing adulterated validation videos.\n",
            "Number of validation data samples: 16\n",
            "Validation Loss: 1.4372, Accuracy: 0.5625\n",
            "Performing test predictions...\n",
            "Processing video: /content/drive/MyDrive/SegmentedTest/cropped_Copy of 65-90 (1).avi, Total frames: 395\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedTest/cropped_Copy of 65-90 (1).avi\n",
            "Test video cropped_Copy of 65-90 (1): Predicted label 0 (0=Pure, 1=Adulterated), Probability: 0.4027\n",
            "Processing video: /content/drive/MyDrive/SegmentedTest/cropped_Copy of 95-110 (1).avi, Total frames: 285\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedTest/cropped_Copy of 95-110 (1).avi\n",
            "Test video cropped_Copy of 95-110 (1): Predicted label 0 (0=Pure, 1=Adulterated), Probability: 0.2688\n",
            "Processing video: /content/drive/MyDrive/SegmentedTest/cropped_Copy of 35-60 (1).avi, Total frames: 486\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedTest/cropped_Copy of 35-60 (1).avi\n",
            "Test video cropped_Copy of 35-60 (1): Predicted label 0 (0=Pure, 1=Adulterated), Probability: 0.0280\n",
            "Processing video: /content/drive/MyDrive/SegmentedTest/cropped_Copy of 35-60.avi, Total frames: 481\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedTest/cropped_Copy of 35-60.avi\n",
            "Test video cropped_Copy of 35-60: Predicted label 0 (0=Pure, 1=Adulterated), Probability: 0.0065\n",
            "Processing video: /content/drive/MyDrive/SegmentedTest/cropped_Copy of 65-90.avi, Total frames: 370\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedTest/cropped_Copy of 65-90.avi\n",
            "Test video cropped_Copy of 65-90: Predicted label 0 (0=Pure, 1=Adulterated), Probability: 0.0069\n",
            "Processing video: /content/drive/MyDrive/SegmentedTest/cropped_Copy of 95-110.avi, Total frames: 450\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedTest/cropped_Copy of 95-110.avi\n",
            "Test video cropped_Copy of 95-110: Predicted label 0 (0=Pure, 1=Adulterated), Probability: 0.0476\n",
            "Processing video: /content/drive/MyDrive/SegmentedTest/cropped_1Copy of 35-60.avi, Total frames: 481\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedTest/cropped_1Copy of 35-60.avi\n",
            "Test video cropped_1Copy of 35-60: Predicted label 0 (0=Pure, 1=Adulterated), Probability: 0.0017\n",
            "Processing video: /content/drive/MyDrive/SegmentedTest/cropped_2Copy of 65-90.avi, Total frames: 486\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedTest/cropped_2Copy of 65-90.avi\n",
            "Test video cropped_2Copy of 65-90: Predicted label 0 (0=Pure, 1=Adulterated), Probability: 0.0084\n",
            "Processing video: /content/drive/MyDrive/SegmentedTest/cropped_5Copy of 95-110.avi, Total frames: 385\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedTest/cropped_5Copy of 95-110.avi\n",
            "Test video cropped_5Copy of 95-110: Predicted label 0 (0=Pure, 1=Adulterated), Probability: 0.0754\n",
            "Processing video: /content/drive/MyDrive/SegmentedTest/cropped_6Copy of 65-90.avi, Total frames: 405\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedTest/cropped_6Copy of 65-90.avi\n",
            "Test video cropped_6Copy of 65-90: Predicted label 0 (0=Pure, 1=Adulterated), Probability: 0.0180\n",
            "Processing video: /content/drive/MyDrive/SegmentedTest/cropped_7Copy of 35-60.avi, Total frames: 491\n",
            "Extracted 10 features from /content/drive/MyDrive/SegmentedTest/cropped_7Copy of 35-60.avi\n",
            "Test video cropped_7Copy of 35-60: Predicted label 0 (0=Pure, 1=Adulterated), Probability: 0.1314\n",
            "Feature extraction, training, validation, and testing completed.\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define base directories and folder names\n",
        "base_dir = '/content/drive/MyDrive'\n",
        "pure_train_folder = os.path.join(base_dir, '/content/drive/MyDrive/SegmentedVideoPT')\n",
        "adulterated_train_folder = os.path.join(base_dir, '/content/drive/MyDrive/SegmentedVideoAT')\n",
        "pure_val_folder = os.path.join(base_dir, '/content/drive/MyDrive/SegmentedVideoPV')\n",
        "adulterated_val_folder = os.path.join(base_dir, '/content/drive/MyDrive/SegmentedVideoAV')\n",
        "test_folder = os.path.join(base_dir, '/content/drive/MyDrive/SegmentedTest')\n",
        "\n",
        "# Create data pool for video paths\n",
        "video_paths = {\n",
        "    'pure_train': [],\n",
        "    'adulterated_train': [],\n",
        "    'pure_val': [],\n",
        "    'adulterated_val': [],\n",
        "    'test': []\n",
        "}\n",
        "\n",
        "# Populate video paths\n",
        "for folder, key in [\n",
        "    (pure_train_folder, 'pure_train'),\n",
        "    (adulterated_train_folder, 'adulterated_train'),\n",
        "    (pure_val_folder, 'pure_val'),\n",
        "    (adulterated_val_folder, 'adulterated_val'),\n",
        "    (test_folder, 'test')\n",
        "]:\n",
        "    if os.path.exists(folder):\n",
        "        for file in os.listdir(folder):\n",
        "            if file.lower().endswith(('.mp4', '.avi', '.mov')):\n",
        "                video_paths[key].append(os.path.join(folder, file))\n",
        "    else:\n",
        "        print(f\"Folder {folder} not found\")\n",
        "\n",
        "# Load pre-trained ViT model and modify to remove classification head\n",
        "model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "model.eval()\n",
        "\n",
        "# Remove the classification head to use the transformer output\n",
        "model.heads = nn.Identity()  # Replace the head with an identity function to get the [CLS] token\n",
        "\n",
        "# Define preprocessing transformations (adjusted for ViT input)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),  # ViT-B/16 expects 224x224 input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
        "])\n",
        "\n",
        "# Function to extract frames and features from a video\n",
        "def extract_frames_and_features(video_path, num_frames=10):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {video_path}\")\n",
        "        return None, None\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(f\"Processing video: {video_path}, Total frames: {total_frames}\")\n",
        "    if total_frames == 0:\n",
        "        print(\"No frames in video.\")\n",
        "        cap.release()\n",
        "        return None, None\n",
        "\n",
        "    frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]\n",
        "    if frame_indices and frame_indices[-1] >= total_frames:\n",
        "        frame_indices[-1] = total_frames - 1\n",
        "    elif not frame_indices and total_frames > 0:\n",
        "        frame_indices = [0]\n",
        "\n",
        "    frames, features = [], []\n",
        "    for idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            input_tensor = preprocess(frame_rgb).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                feature = model(input_tensor).squeeze().numpy()  # 768-dimensional [CLS] token feature per frame\n",
        "            frames.append(frame_rgb)\n",
        "            features.append(feature)\n",
        "        else:\n",
        "            print(f\"Failed to read frame {idx} from video {video_path}\")\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Extracted {len(features)} features from {video_path}\")\n",
        "    return frames, features\n",
        "\n",
        "# Function to average features across frames\n",
        "def average_features(features):\n",
        "    if not features:\n",
        "        return None\n",
        "    feature_matrix = np.array(features)  # Shape should be (num_frames, feature_dim)\n",
        "    averaged_feature = np.mean(feature_matrix, axis=0)  # Average across frames\n",
        "    return averaged_feature\n",
        "\n",
        "# Define sigmoid layer for binary classification\n",
        "class SigmoidClassifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SigmoidClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)  # Single output for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "# Prepare training data\n",
        "train_data = []\n",
        "train_labels = []\n",
        "print(\"Preparing training data...\")\n",
        "for label, paths in [('pure', video_paths['pure_train']), ('adulterated', video_paths['adulterated_train'])]:\n",
        "    print(f\"Processing {label} training videos...\")\n",
        "    for video_path in paths:\n",
        "        frames, features = extract_frames_and_features(video_path)\n",
        "        if features:\n",
        "            averaged_feature = average_features(features)\n",
        "            train_data.append(averaged_feature)\n",
        "            train_labels.append(0 if label == 'pure' else 1)\n",
        "    print(f\"Finished processing {label} training videos.\")\n",
        "\n",
        "print(f\"Number of training data samples: {len(train_data)}\")\n",
        "if not train_data:\n",
        "    print(\"No training data collected. Exiting.\")\n",
        "else:\n",
        "    train_data = np.array(train_data)\n",
        "    train_labels = np.array(train_labels)\n",
        "\n",
        "    # Normalize training data\n",
        "    # train_data = (train_data - train_data.min()) / (train_data.max() - train_data.min() + 1e-8)\n",
        "    # print(f\"Train data shape after normalization: {train_data.shape}\")\n",
        "    # print(f\"Train data min: {train_data.min()}, max: {train_data.max()}\")\n",
        "\n",
        "    # Initialize and train the classifier\n",
        "    input_dim = train_data.shape[1]\n",
        "    classifier = SigmoidClassifier(input_dim)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
        "\n",
        "    # Convert data to tensors\n",
        "    train_data_tensor = torch.FloatTensor(train_data)\n",
        "    train_labels_tensor = torch.FloatTensor(train_labels).reshape(-1, 1)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 200\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = classifier(train_data_tensor)\n",
        "        loss = criterion(outputs, train_labels_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Validation\n",
        "    val_data = []\n",
        "    val_labels = []\n",
        "    print(\"Preparing validation data...\")\n",
        "    for label, paths in [('pure', video_paths['pure_val']), ('adulterated', video_paths['adulterated_val'])]:\n",
        "        print(f\"Processing {label} validation videos...\")\n",
        "        for video_path in paths:\n",
        "            frames, features = extract_frames_and_features(video_path)\n",
        "            if features:\n",
        "                averaged_feature = average_features(features)\n",
        "                val_data.append(averaged_feature)\n",
        "                val_labels.append(0 if label == 'pure' else 1)\n",
        "        print(f\"Finished processing {label} validation videos.\")\n",
        "\n",
        "    print(f\"Number of validation data samples: {len(val_data)}\")\n",
        "    if not val_data:\n",
        "        print(\"No validation data collected. Skipping validation.\")\n",
        "    else:\n",
        "        val_data = np.array(val_data)\n",
        "        val_labels = np.array(val_labels)\n",
        "        # val_data = (val_data - val_data.min()) / (val_data.max() - val_data.min() + 1e-8)\n",
        "        # print(f\"Validation data shape after normalization: {val_data.shape}\")\n",
        "        # print(f\"Validation data min: {val_data.min()}, max: {val_data.max()}\")\n",
        "        val_data_tensor = torch.FloatTensor(val_data)\n",
        "        val_labels_tensor = torch.FloatTensor(val_labels).reshape(-1, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_outputs = classifier(val_data_tensor)\n",
        "            val_loss = criterion(val_outputs, val_labels_tensor)\n",
        "            val_accuracy = ((torch.sigmoid(val_outputs).round() == val_labels_tensor).float().mean()).item()\n",
        "            print(f'Validation Loss: {val_loss.item():.4f}, Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # Test prediction\n",
        "    print(\"Performing test predictions...\")\n",
        "    for video_path in video_paths['test'][:12]:\n",
        "        frames, features = extract_frames_and_features(video_path)\n",
        "        if features:\n",
        "            averaged_feature = average_features(features)\n",
        "            test_data_tensor = torch.FloatTensor(averaged_feature).reshape(1, -1)\n",
        "            with torch.no_grad():\n",
        "                prediction = torch.sigmoid(classifier(test_data_tensor))\n",
        "                predicted_label = 0 if prediction.item() < 0.5 else 1\n",
        "                probability = prediction.item()\n",
        "            video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "            print(f'Test video {video_name}: Predicted label {predicted_label} (0=Pure, 1=Adulterated), Probability: {probability:.4f}')\n",
        "        else:\n",
        "            video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "            print(f'Could not extract features for test video {video_name}. Skipping prediction.')\n",
        "\n",
        "    print(\"Feature extraction, training, validation, and testing completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ETyto8MaWmpl"
      },
      "outputs": [],
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nW5cN7Ibobmd"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define base directories and folder names\n",
        "base_dir = '/content/drive/MyDrive'\n",
        "pure_train_folder = os.path.join(base_dir, 'SegmentedVideoPT')\n",
        "adulterated_train_folder = os.path.join(base_dir, 'SegmentedVideoAT')\n",
        "pure_val_folder = os.path.join(base_dir, 'SegmentedVideoPV')\n",
        "adulterated_val_folder = os.path.join(base_dir, 'SegmentedVideoAV')\n",
        "test_folder = os.path.join(base_dir, 'SegmentedTest')\n",
        "\n",
        "# Create data pool for video paths\n",
        "video_paths = {\n",
        "    'pure_train': [],\n",
        "    'adulterated_train': [],\n",
        "    'pure_val': [],\n",
        "    'adulterated_val': [],\n",
        "    'test': []\n",
        "}\n",
        "\n",
        "# Populate video paths\n",
        "for folder, key in [\n",
        "    (pure_train_folder, 'pure_train'),\n",
        "    (adulterated_train_folder, 'adulterated_train'),\n",
        "    (pure_val_folder, 'pure_val'),\n",
        "    (adulterated_val_folder, 'adulterated_val'),\n",
        "    (test_folder, 'test')\n",
        "]:\n",
        "    if os.path.exists(folder):\n",
        "        for file in os.listdir(folder):\n",
        "            if file.lower().endswith(('.mp4', '.avi', '.mov')):\n",
        "                video_paths[key].append(os.path.join(folder, file))\n",
        "    else:\n",
        "        print(f\"Folder {folder} not found\")\n",
        "\n",
        "# Define a simple 3D CNN model\n",
        "class Simple3DCNN(nn.Module):\n",
        "    def __init__(self, num_frames=10):\n",
        "        super(Simple3DCNN, self).__init__()\n",
        "        self.num_frames = num_frames\n",
        "        self.conv1 = nn.Conv3d(3, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
        "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
        "        self.conv3 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
        "        self.fc1 = nn.Linear(128 * (num_frames // 4) * (224 // 8) * (224 // 8), 512)\n",
        "        self.fc2 = nn.Linear(512, 1)  # Output logit for binary classification\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu(self.conv1(x)))\n",
        "        x = self.pool2(self.relu(self.conv2(x)))\n",
        "        x = self.pool3(self.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the 3D CNN model\n",
        "model = Simple3DCNN(num_frames=10)\n",
        "model = model.cuda() if torch.cuda.is_available() else model  # Move to GPU if available\n",
        "model.train()  # Set to training mode for initialization\n",
        "\n",
        "# Define preprocessing transformations for 3D CNN\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),  # 3D CNN expects 224x224 spatial input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet-like normalization\n",
        "])\n",
        "\n",
        "# Function to extract frames and prepare video clip for 3D CNN\n",
        "def extract_video_clip(video_path, num_frames=10):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {video_path}\")\n",
        "        return None\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(f\"Processing video: {video_path}, Total frames: {total_frames}\")\n",
        "    if total_frames == 0:\n",
        "        print(\"No frames in video.\")\n",
        "        cap.release()\n",
        "        return None\n",
        "\n",
        "    frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]\n",
        "    if frame_indices and frame_indices[-1] >= total_frames:\n",
        "        frame_indices[-1] = total_frames - 1\n",
        "    elif not frame_indices and total_frames > 0:\n",
        "        frame_indices = [0]\n",
        "\n",
        "\n",
        "    clip = []\n",
        "    for idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            clip.append(preprocess(frame_rgb))\n",
        "        else:\n",
        "            print(f\"Failed to read frame {idx} from video {video_path}\")\n",
        "\n",
        "    cap.release()\n",
        "    if len(clip) < num_frames:\n",
        "        # Pad with last frame if needed\n",
        "        if clip: # Check if clip is not empty\n",
        "            clip += [clip[-1]] * (num_frames - len(clip))\n",
        "        else: # If clip is empty, return None\n",
        "            return None\n",
        "\n",
        "\n",
        "    # Shape: (T, C, H, W) to (C, T, H, W)\n",
        "    return torch.stack(clip).permute(1, 0, 2, 3)\n",
        "\n",
        "\n",
        "# Prepare training data and train the model\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Prepare data in batches\n",
        "train_clips = []\n",
        "train_labels_list = []\n",
        "for label, paths in [('pure', video_paths['pure_train']), ('adulterated', video_paths['adulterated_train'])]:\n",
        "    print(f\"Processing {label} training videos...\")\n",
        "    for video_path in paths:\n",
        "        video_clip = extract_video_clip(video_path, num_frames=10)\n",
        "        if video_clip is not None:\n",
        "            train_clips.append(video_clip)\n",
        "            train_labels_list.append(0 if label == 'pure' else 1)\n",
        "    print(f\"Finished processing {label} training videos.\")\n",
        "\n",
        "print(f\"Number of training data samples: {len(train_clips)}\")\n",
        "if not train_clips:\n",
        "    print(\"No training data collected. Exiting.\")\n",
        "else:\n",
        "    train_dataset = TensorDataset(torch.stack(train_clips), torch.FloatTensor(train_labels_list).reshape(-1, 1))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "    # Train the model\n",
        "    criterion = nn.BCEWithLogitsLoss().cuda() if torch.cuda.is_available() else nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    num_epochs = 6\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_clips, batch_labels in train_loader:\n",
        "            batch_clips = batch_clips.cuda() if torch.cuda.is_available() else batch_clips\n",
        "            batch_labels = batch_labels.cuda() if torch.cuda.is_available() else batch_labels\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_clips)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / len(train_loader):.4f}')\n",
        "\n",
        "    # Validation\n",
        "    val_data = []\n",
        "    val_labels = []\n",
        "    print(\"Preparing validation data...\")\n",
        "    for label, paths in [('pure', video_paths['pure_val']), ('adulterated', video_paths['adulterated_val'])]:\n",
        "        print(f\"Processing {label} validation videos...\")\n",
        "        for video_path in paths:\n",
        "            video_clip = extract_video_clip(video_path, num_frames=10)\n",
        "            if video_clip is not None:\n",
        "                val_data.append(video_clip)\n",
        "                val_labels.append(0 if label == 'pure' else 1)\n",
        "        print(f\"Finished processing {label} validation videos.\")\n",
        "\n",
        "    print(f\"Number of validation data samples: {len(val_data)}\")\n",
        "    if not val_data:\n",
        "        print(\"No validation data collected. Skipping validation.\")\n",
        "    else:\n",
        "        val_labels_tensor = torch.FloatTensor(val_labels).reshape(-1, 1).cuda() if torch.cuda.is_available() else torch.FloatTensor(val_labels).reshape(-1, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            val_outputs = model(torch.stack(val_data).cuda() if torch.cuda.is_available() else torch.stack(val_data)) # Pass the stacked video clips to the model\n",
        "            val_loss = criterion(val_outputs, val_labels_tensor)\n",
        "            val_accuracy = ((torch.sigmoid(val_outputs).round() == val_labels_tensor).float().mean()).item()\n",
        "            print(f'Validation Loss: {val_loss.item():.4f}, Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # Test prediction\n",
        "    print(\"Performing test predictions...\")\n",
        "    for video_path in video_paths['test'][:4]:\n",
        "        video_clip = extract_video_clip(video_path, num_frames=10)\n",
        "        if video_clip is not None:\n",
        "            test_data_tensor = video_clip.unsqueeze(0).cuda() if torch.cuda.is_available() else video_clip.unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                prediction = torch.sigmoid(model(test_data_tensor))\n",
        "                predicted_label = 0 if prediction.item() < 0.5 else 1\n",
        "                probability = prediction.item()\n",
        "            video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "            print(f'Test video {video_name}: Predicted label {predicted_label} (0=Pure, 1=Adulterated), Probability: {probability:.4f}')\n",
        "        else:\n",
        "            video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "            print(f'Could not extract features for test video {video_name}. Skipping prediction.')\n",
        "\n",
        "    print(\"Feature extraction, training, validation, and testing completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3ce1e2d9"
      },
      "outputs": [],
      "source": [
        "# Uninstall existing PyTorch and torchvision\n",
        "!pip uninstall torch torchvision -y\n",
        "\n",
        "# Install specific versions known for better stability\n",
        "!pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}